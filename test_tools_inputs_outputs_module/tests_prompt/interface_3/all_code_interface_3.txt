Assume you are an experienced tester. I am going to give you python functions and you are going to give me the inputs to test this function. The inputs are provided in the form of actions. You have to provide three tests for each function within the same file with the most number of parameters as possible (if applicable). However, you have to follow the format that I provide to you below.

# Format:
{
    "env": "wiki_pages",
    "interface_num": 3,
    "task": {
        "actions": [
            {
                "name": "discover_user_entities",
                "arguments": {
                    "entity_type": "users",
                    "filters": {
                        "email": "sarahcampos144@hotmail.com"
                    }
                }
            },
            {
                "name": "discover_user_entities",
                "arguments": {
                    "entity_type": "users",
                    "filters": {
                        "email": "sarahcampos144@hotmail.com"
                    }
                }
            }
        ]
    }
}

where the name of action is the function name and the arguments are the parameters of the function. You have to provide multiple actions for a single file in a single test if applicable. 

# Functions:
import json
from typing import Any, Dict, Optional
from tau_bench.envs.tool import Tool

class AttainPermission(Tool):
    @staticmethod
    def invoke(
        data: Dict[str, Any],
        filters: Dict[str, Any]
    ) -> str:
        """
        Attain permission details based on filters (Fibery logic).
        """
        # Note: The doc specifies a 'filters' dict parameter, 
        # but your previous code flattened them. 
        # Adhering to the doc strictly:
        permission_id = filters.get("permission_id")
        content_id = filters.get("content_id")
        content_type = filters.get("content_type")
        user_id = filters.get("user_id")
        operation = filters.get("operation")
        granted_by = filters.get("granted_by")

        permissions = data.get("permissions", {})
        results = []
        
        for perm_id, perm_data in permissions.items():
            match = True
            if permission_id and perm_id != permission_id: match = False
            if content_id and perm_data.get("content_id") != content_id: match = False
            if content_type and perm_data.get("content_type") != content_type: match = False
            if user_id and perm_data.get("user_id") != user_id: match = False
            if operation and perm_data.get("operation") != operation: match = False
            if granted_by and perm_data.get("granted_by") != granted_by: match = False
            
            if match:
                results.append({**perm_data, "permission_id": perm_id})
        
        return json.dumps({"success": True, "count": len(results), "results": results})

    @staticmethod
    def get_info() -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": "attain_permission", 
                "description": "Attain permissions based on filters.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "filters": {
                            "type": "object",
                            "properties": {
                                "permission_id": {"type": "string"},
                                "content_id": {"type": "string"},
                                "content_type": {"type": "string", "enum": ["space", "page"]},
                                "user_id": {"type": "string"},
                                "operation": {"type": "string", "enum": ["view", "edit", "delete", "create", "admin", "restrict_other_users"]},
                                "granted_by": {"type": "string"}
                            }
                        }
                    },
                    "required": ["filters"]
                }
            }
        }


# -----------------

import json
from typing import Any, Dict, Optional
from tau_bench.envs.tool import Tool

class DiscoverWhiteboardView(Tool):
    @staticmethod
    def invoke(
        data: Dict[str, Any],
        whiteboard_view_id: Optional[str] = None,
        host_workspace_id: Optional[str] = None,
        host_document_id: Optional[str] = None,
        title: Optional[str] = None,
        status: Optional[str] = None,
        created_by: Optional[str] = None,
        updated_by: Optional[str] = None
    ) -> str:
        """
        Discover whiteboard views in the Fibery workspace.
        Maps to Confluence whiteboards table.
        """
        whiteboards = data.get("whiteboards", {})
        results = []
        
        for wid, w_data in whiteboards.items():
            match = True
            
            # ID Filter
            if whiteboard_view_id and wid != whiteboard_view_id:
                match = False
            
            # Title Filter
            if title and w_data.get("title") != title:
                match = False
            
            # Parent Filters (Workspace vs Document)
            if host_workspace_id and w_data.get("host_space_id") != host_workspace_id:
                match = False
            if host_document_id and w_data.get("host_page_id") != host_document_id:
                match = False
                
            # Metadata Filters
            if status and w_data.get("status") != status:
                match = False
            if created_by and w_data.get("created_by") != created_by:
                match = False
            if updated_by and w_data.get("updated_by") != updated_by:
                match = False
            
            if match:
                results.append({
                    "whiteboard_view_id": wid,
                    "title": w_data.get("title"),
                    "host_workspace_id": w_data.get("host_space_id"),
                    "host_document_id": w_data.get("host_page_id"),
                    "content": w_data.get("content"),
                    "status": w_data.get("status"),
                    "created_by": w_data.get("created_by"),
                    "created_at": w_data.get("created_at"),
                    "updated_by": w_data.get("updated_by"),
                    "updated_at": w_data.get("updated_at")
                })
        
        return json.dumps({
            "success": True,
            "count": len(results),
            "results": results
        })

    @staticmethod
    def get_info() -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": "discover_whiteboard_view", # CORRECTED NAME
                "description": "Discover whiteboard views based on filters. Can search by whiteboard_view_id, title, host_workspace_id, host_document_id, status ('current', 'archived', 'deleted', 'locked'), created_by, or updated_by. Returns details including the view ID, title, parent container (workspace or document), content, and metadata.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "whiteboard_view_id": {
                            "type": "string",
                            "description": "Unique identifier of the whiteboard view"
                        },
                        "title": {
                            "type": "string",
                            "description": "Filter by whiteboard title"
                        },
                        "host_workspace_id": {
                            "type": "string",
                            "description": "Filter by the parent workspace ID (if hosted in a workspace)"
                        },
                        "host_document_id": {
                            "type": "string",
                            "description": "Filter by the parent document ID (if hosted in a document)"
                        },
                        "status": {
                            "type": "string",
                            "description": "Filter by status: 'current', 'archived', 'deleted', 'locked'",
                            "enum": ["current", "archived", "deleted", "locked"]
                        },
                        "created_by": {
                            "type": "string",
                            "description": "Filter by creator user ID"
                        },
                        "updated_by": {
                            "type": "string",
                            "description": "Filter by last updater user ID"
                        }
                    },
                    "required": []
                }
            }
        }

# -----------------

ALL_TOOLS_INTERFACE_3 = []


# -----------------

import json
from typing import Any, Dict, Optional
from tau_bench.envs.tool import Tool

class InsertWhiteboardView(Tool):
    @staticmethod
    def invoke(
        data: Dict[str, Any],
        title: str,
        created_by: str,
        host_workspace_id: Optional[str] = None,
        host_document_id: Optional[str] = None,
        content: Optional[str] = None,
        status: str = "current"
    ) -> str:
        """
        Create a new whiteboard view in Fibery.
        Maps to Confluence whiteboards table.
        """
        def generate_id(table: Dict[str, Any]) -> str:
            if not table:
                return "1"
            return str(max(int(k) for k in table.keys()) + 1)

        timestamp = "2025-10-01T00:00:00"
        whiteboards = data.get("whiteboards", {})
        spaces = data.get("spaces", {})
        pages = data.get("pages", {})
        users = data.get("users", {})

        # Validate that either host_workspace_id OR host_document_id is provided (mutually exclusive)
        if not host_workspace_id and not host_document_id:
            return json.dumps({
                "success": False,
                "error": "Either host_workspace_id or host_document_id must be provided"
            })

        if host_workspace_id and host_document_id:
            return json.dumps({
                "success": False,
                "error": "Only one of host_workspace_id or host_document_id should be provided"
            })

        # Validate host workspace if provided
        if host_workspace_id:
            if host_workspace_id not in spaces:
                return json.dumps({
                    "success": False,
                    "error": f"Workspace with ID '{host_workspace_id}' not found"
                })

        # Validate host document if provided
        if host_document_id:
            if host_document_id not in pages:
                return json.dumps({
                    "success": False,
                    "error": f"Document with ID '{host_document_id}' not found"
                })

        # Validate user exists and is active
        if created_by not in users:
            return json.dumps({
                "success": False,
                "error": f"User with ID '{created_by}' not found"
            })
        
        user = users[created_by]
        if user.get("status") != "active":
            return json.dumps({
                "success": False,
                "error": f"User with ID '{created_by}' is not active"
            })

        # Validate status
        valid_statuses = ["current", "archived", "deleted", "locked"]
        if status not in valid_statuses:
            return json.dumps({
                "success": False,
                "error": f"Invalid status. Must be one of: {', '.join(valid_statuses)}"
            })

        # Generate new whiteboard ID
        new_whiteboard_id = generate_id(whiteboards)

        # Create new whiteboard
        new_whiteboard = {
            "whiteboard_id": new_whiteboard_id,
            "title": title,
            "host_space_id": host_workspace_id,
            "host_page_id": host_document_id,
            "content": content,
            "status": status,
            "created_by": created_by,
            "created_at": timestamp,
            "updated_by": created_by,
            "updated_at": timestamp
        }

        whiteboards[new_whiteboard_id] = new_whiteboard

        return json.dumps({
            "success": True,
            "whiteboard_view_id": new_whiteboard_id,
            "whiteboard_data": new_whiteboard
        })

    @staticmethod
    def get_info() -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": "insert_whiteboard_view",
                "description": "Create a new collaborative whiteboard view. Whiteboards can be hosted at workspace level or document level (mutually exclusive). Status options: 'current', 'archived', 'deleted', 'locked'.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "title": {
                            "type": "string",
                            "description": "Title of the whiteboard (required)"
                        },
                        "created_by": {
                            "type": "string",
                            "description": "User ID of the whiteboard creator (required)"
                        },
                        "host_workspace_id": {
                            "type": "string",
                            "description": "ID of the workspace hosting the whiteboard (optional, mutually exclusive with host_document_id)"
                        },
                        "host_document_id": {
                            "type": "string",
                            "description": "ID of the document hosting the whiteboard (optional, mutually exclusive with host_workspace_id)"
                        },
                        "content": {
                            "type": "string",
                            "description": "Content of the whiteboard (optional)"
                        },
                        "status": {
                            "type": "string",
                            "description": "Whiteboard status: 'current', 'archived', 'deleted', 'locked' (defaults to 'current')",
                            "enum": ["current", "archived", "deleted", "locked"]
                        }
                    },
                    "required": ["title", "created_by"]
                }
            }
        }


# -----------------

import json
from typing import Any, Dict, Optional
from tau_bench.envs.tool import Tool

class InsertDocument(Tool):
    @staticmethod
    def invoke(
        data: Dict[str, Any],
        title: str,
        workspace_id: str,
        created_by: str,
        parent_document_id: Optional[str] = None,
        body_storage: Optional[str] = None,
        status: str = "current"
    ) -> str:
        """
        Create a new document in the Fibery workspace.
        Maps to Confluence pages table.
        """
        def generate_id(table: Dict[str, Any]) -> str:
            if not table:
                return "1"
            return str(max(int(k) for k in table.keys()) + 1)

        timestamp = "2025-10-01T00:00:00"
        pages = data.get("pages", {})
        spaces = data.get("spaces", {})
        users = data.get("users", {})

        # Validate workspace (space) exists
        if workspace_id not in spaces:
            return json.dumps({
                "success": False,
                "error": f"Workspace with ID '{workspace_id}' not found"
            })

        # Validate user exists and is active
        if created_by not in users:
            return json.dumps({
                "success": False,
                "error": f"User with ID '{created_by}' not found"
            })
        
        user = users[created_by]
        if user.get("status") != "active":
            return json.dumps({
                "success": False,
                "error": f"User with ID '{created_by}' is not active"
            })

        # Validate parent document if provided
        if parent_document_id:
            if parent_document_id not in pages:
                return json.dumps({
                    "success": False,
                    "error": f"Parent document with ID '{parent_document_id}' not found"
                })
            
            parent_page = pages[parent_document_id]
            if parent_page.get("space_id") != workspace_id:
                return json.dumps({
                    "success": False,
                    "error": "Parent document must be in the same workspace"
                })

        # Validate status
        valid_statuses = ["current", "draft", "locked", "archived", "deleted"]
        if status not in valid_statuses:
            return json.dumps({
                "success": False,
                "error": f"Invalid status. Must be one of: {', '.join(valid_statuses)}"
            })

        # Check for duplicate title in same hierarchy level
        for existing_page in pages.values():
            if (existing_page.get("title") == title and 
                existing_page.get("space_id") == workspace_id and
                existing_page.get("parent_page_id") == parent_document_id):
                return json.dumps({
                    "success": False,
                    "error": f"Document with title '{title}' already exists at this hierarchy level"
                })

        # Generate new document ID
        new_page_id = generate_id(pages)

        # Create new document (page)
        new_page = {
            "page_id": new_page_id,
            "title": title,
            "space_id": workspace_id,
            "parent_page_id": parent_document_id,
            "body_storage": body_storage,
            "status": status,
            "created_by": created_by,
            "created_at": timestamp,
            "updated_by": created_by,
            "updated_at": timestamp
        }

        pages[new_page_id] = new_page

        return json.dumps({
            "success": True,
            "document_id": new_page_id,
            "document_data": new_page
        })

    @staticmethod
    def get_info() -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": "insert_document",
                "description": "Create a new document in a Fibery workspace. Documents are hierarchical and can have parent-child relationships. Status options: 'current', 'draft', 'locked', 'archived', 'deleted'.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "title": {
                            "type": "string",
                            "description": "Title of the document (required)"
                        },
                        "workspace_id": {
                            "type": "string",
                            "description": "ID of the workspace where the document will be created (required)"
                        },
                        "created_by": {
                            "type": "string",
                            "description": "User ID of the document creator (required)"
                        },
                        "parent_document_id": {
                            "type": "string",
                            "description": "ID of the parent document for child documents (optional)"
                        },
                        "body_storage": {
                            "type": "string",
                            "description": "Content of the document (optional)"
                        },
                        "status": {
                            "type": "string",
                            "description": "Document status: 'current', 'draft', 'locked', 'archived', 'deleted' (defaults to 'current')",
                            "enum": ["current", "draft", "locked", "archived", "deleted"]
                        }
                    },
                    "required": ["title", "workspace_id", "created_by"]
                }
            }
        }


# Policy:
FIBERY MANAGEMENT SOPs – Technical Documentation
Current date: 27th Nov, 2025
INTRODUCTION
This policy establishes standardized procedures for managing the workspace environment, focusing on document creation, hierarchy management, and permission governance. The policy ensures consistent operations across the Fibery deployment while maintaining security, compliance, and collaborative efficiency.
GENERAL OPERATING PRINCIPLES
• You must not perform or provide any actions, data, or recommendations that are not derived from system tools or authorized records.
 • All operations must be executed strictly through approved Fibery system tools under role-based permissions.
 • You must deny user requests that violate this policy.
 • Each Standard Operating Procedure (SOP) is self-contained and designed for single-turn execution, ensuring end-to-end completion within one interaction.
 • Every SOP provides clear steps for proceeding when conditions are met and explicit halt instructions with error reporting when conditions are not met.
CRITICAL HALT AND TRANSFER CONDITIONS
You must immediately halt the procedure and initiate a escalate_to_human if any of the following critical conditions occur:
Authorization & Credentials Issues:
 Unauthorized user, missing/invalid credentials, or permission checks failing (including insufficient authorization, minimum access violations, or permission elevation exceeding authority).


System or Tool Failures:
 Any system tool not responding, returning errors, or encountering exceptions that prevent safe continuation.


Missing or Invalid Entities:
 Required entities (document, workspace, whiteboard, type, embed block, attachments) are missing, invalid, locked, archived, protected, or have invalid status/metadata/properties.


Hierarchy & Parent Entity Problems:
 Parent document/workspace not found, hierarchy inconsistencies, or orphaned content that cannot be reassigned.


Conflicts & Integrity Errors:
 Name conflicts, type name conflicts, invalid type properties or field definitions, or invalid reference identifiers/display modes.


Compliance, Legal, or Policy Restrictions:
 Legal hold, compliance restrictions, data integrity check failures, or operations violating governance requirements.


Dependency & Workflow Constraints:
 Active relations, views, dependencies, or workflows that prevent deletion or modification.


Template & Resource Issues:
 Missing templates for template-based operations, missing whiteboards/embed blocks/attachments, or invalid label/tag fields or values.


File & Upload Failures:
 File upload failures, invalid file format/size, or issues with attachment insertion/removal/update.


Operation & Logging Failures:
 Entity creation/update/deletion failures or audit trail logging failures.
If any external integration (e.g., database or API) fails, you must halt and provide appropriate error messaging.
Only when none of these conditions are triggered should you proceed to finalize the procedure.

STANDARD OPERATING PROCEDURES (SOPs)
1. Create Document
Steps:
Determine whether the new document is a child document
Retrieve user credentials and validate that the user is active using retrieve_user.
If creating within existing parent document:
 • Retrieve parent document details using retrieve_document.
 • Verify user permission to create on the parent document by checking attain_permission status for "create" authorization.
If creating root-level document in workspace:
 • Retrieve Workspace details using retrieve_workspace.
 • Verify workspace-level "create" permissions using attain_permission.
Retrieve documents within the same hierarchy level (same parent or same workspace root) using retrieve_document with a title filter (title is unique) to ensure the new document does not already exist.
Upon authorization confirmation:
Create a new document using insert_document.
Establish the first version using establish_document_version.
Execute produce_access_to_app  to set creator as document admin.


2. Update Document Content
Steps:
Retrieve user authentication and verify active status via retrieve_user.
Fetch target document using retrieve_document .
Validate "edit" permissions by checking attain_permission status.
Use fetch_descendant_document with the target document ID to obtain all child and nested child documents.
If relocating document to different workspace:
 • Retrieve target workspace using retrieve_workspace.
 • Verify destination permissions via attain_permission.
 • For each descendant document found:
	 ○ Update workspace reference using alter_document.
 	 ○ Execute alter_access_to_app for new workspace permissions.
If modifying parent document:
 • Retrieve a new parent document using retrieve_document.
 • Update parent relationship using alter_document.
 • For descendant permissions: alter_access_to_app to realign descendant permissions with the updated hierarchy.
If updating document title, query workspace using retrieve_document with title field to check uniqueness.
Apply content modifications via alter_document.
Create a version snapshot using establish_document_version.
3. Remove Document 
Steps:
Retrieve target document via retrieve_document and verify it is not "locked" or "archived."
Authenticate user credentials using retrieve_user confirming active status.
Validate "delete" authorization through attain_permission on permission field.
Identify dependent documents using fetch_descendant_document with parent reference.
If dependencies exist:
 • Retrieve immediate children with discover_direct_children.
 • Locate parent document using attain_ancestors for ancestor reference.
 • If parent exists:
Reassign children to grandparent using alter_document for each child.
Update permission inheritance using alter_access_to_app.
 • If no parent (root document):
 		○ Promote children to root level via alter_document removing parent reference.
○ Establish independent permission using alter_access_to_app.
Create a final version snapshot using establish_document_version.
For soft deletion:
 • Set document status to "archived" using alter_document.
For hard deletion:
 • Retrieve all versions using collect_document_versions filtered by document ID.
 • Delete version blocks using eject_document_version for each version.
 • Execute eject_document for permanent removal.
4. Modify Document Access Permissions
Steps:
Retrieve target document using retrieve_document.
Identify users involved:
Current user (permission modifier) via retrieve_user.
 Target user (permission recipient) via retrieve_user.
Verify both users have active status.
Validate current user authority via attain_permission for "admin" or "restrict_other_users" role.
Retrieve target user's existing permissions using attain_permission.
Determine permission modification type:
 • For permission elevation:
 	○ Verify change doesn't exceed current user's own permissions.
 • For permission reduction:
 	○ Ensure minimum access requirements are maintained and Verify no active workflows depend on access
Apply permission changes:
 • Execute produce_access_to_app with new permission level.
 • Execute alter_access_to_app for updating/deleting permissions.
If document has descendants:
 Query descendants via fetch_descendant_document with parent reference.
For each descendant requiring cascade:
Execute produce_access_to_app with new permission level and maintain the inheritance.
Add document version using establish_document_version
5. Create Collaborative Whiteboard
Steps:
Determine whiteboard hosting location (workspace or document level).
Retrieve hosting entity:
 • For workspace hosting: use retrieve_workspace.
 • For document hosting: use retrieve_document.
 • Confirm entity exists and is active.
Validate user retrieve_user ensuring active status.
Verify "create" permissions using attain_permission. 
If workspace hosted:
Create a whiteboard view using insert_whiteboard_view.
If document hosted:
Create a whiteboard view using insert_whiteboard_view.
Add a document version using establish_document_version.
6. Update Collaborative Whiteboard
Steps:
Retrieve target whiteboard details using discover_whiteboard_view.
Identify hosting (workspace or document) and verify existence using retrieve_workspace or retrieve_document.
Authenticate user via retrieve_user and confirm active status.
Validate "edit" authorization using attain_permission against the host.
If permissions valid: Apply updates using alter_whiteboard_view.
If the whiteboard is hosted in a document:
Locate the host document using retrieve_document.
Record a version host document using establish_document_version to signify state change.
7. Remove Collaborative Whiteboard
Steps:
Retrieve hosting document using retrieve_document.
Authenticate user via retrieve_user.
Verify "delete" permissions using attain_permission.
Identify all embed block reference to the whiteboard:
 • Use retrieve_entity to find documents containing the whiteboard ID.
For each reference found:
 • Execute eject_embed_block to cleanly remove.
Add a document version using establish_document_version.
Execute eject_whiteboard_view to remove the whiteboard view.
8. Embed Resource
Steps:
Retrieve target document via retrieve_document.
Determine the embed block reference entity type:
If the entity type is “Document”, retrieve the page details using retrieve_document.
If the entity is external, proceed to step 3.
Otherwise:
retrieve the entity using retrieve_entity.
Then retrieve the hosting space/page of the entity using retrieve_workspace or retrieve_document.
Authenticate user via retrieve_user and verify the user exists and with “active” status.
Verify "edit" permissions on the document using attain_permission.
Execute embedding create_embed_block  providing the resource identifier.
Create a document version snapshot using establish_document_version
9. Remove Embedded Resource
Steps:
Identify the target embed block using retrieve_entity.
Determine the embed block reference entity type:
If the entity type is “Document”, retrieve the page details using retrieve_document.
If the entity is external, proceed to step 3.
Otherwise:
retrieve the entity using retrieve_entity.
Then retrieve the hosting space/page of the entity using retrieve_workspace or retrieve_document.
Authenticate user via retrieve_user and verify the user exists and with “active” status.
Verify "edit" permissions on the host document using attain_permission.
Execute eject_embed_block to remove the component.
Create a document version snapshot using establish_document_version.
10. Modify Embedded Resource
Steps:
Retrieve target embed block using retrieve_entity.
Retrieve host document via retrieve_document.
Authenticate user via retrieve_user and verify the user exists and with “active” status.
Verify "edit" permissions using attain_permission.
Execute alter_embed_block for updating embed block.
Record change in document via establish_document_version.
11. Initialize New Data Type
Steps:
Identify target Workspace using retrieve_workspace.
Authenticate user via retrieve_user and verify the user exists and with “active” status.
Verify "edit" permissions using attain_permission.
To create a new data type execute insert_datatype and execute.
Execute create_embed_block for linking into whiteboard view.
12. Remove Data Type
Steps:
Retrieve the hosting workspace/document and confirm that the workspace or document exists using retrieve_workspace or retrieve_document respectively.
Retrieve Data Type using retrieve_datatype.
Authenticate user via retrieve_user and verify the user exists and with “active” status.
Verify "delete" permissions using attain_permission.
Recursive Cleanup:
 • Execute eject_embed_block for  deleting embed block references.
 • Execute eject_datatype to remove data type.
13. Update Data Type
Steps:
Retrieve  Data Type definition using retrieve_datatype.
Authenticate user via retrieve_user and verify the user exists and with “active” status.
Verify "edit" permissions using attain_permission.
Execute alter_datatype for changes.


14. Add File or Tag
Steps:
Retrieve target document using retrieve_document.
Authenticate user via retrieve_user and verify the user exists and with “active” status.
Verify "edit" permissions using attain_permission.
If Adding Attachment:
Execute insert_file to store the binary data.
If Adding Label:
Execute insert_tag to assign the label to the page.
15. Remove File or Tag
Steps:
Retrieve target document using retrieve_document.
Authenticate user via retrieve_user and verify the user exists and with “active” status.
Verify "edit" permissions using attain_permission.
Retrieve the tag or file details using retrieve_entity.
If Removing Attachment:
Execute eject_file to remove it.
If Removing Tag:
Detach label from the page using eject_tag.


16. Update File or Tag
Steps:
Retrieve target document using retrieve_document.
Authenticate user via retrieve_user and verify the user exists and with “active” status.
Verify "edit" permissions using attain_permission.
Retrieve the tag or file details using retrieve_entity.
If Updating Attachment:
Execute alter_file to update a file.
If Updating Tag:
Execute alter_tag replacing the old value with the new value.



# Database Schema:
// DBML Schema for Confluence Content Management System
// Based on actual Confluence database table structure

// ============================================================================
// CORE CONTENT TYPES
// ============================================================================

Table pages {
  page_id varchar(50) [primary key]
  // page_number varchar(50) [not null, unique] // e.g., PAGE0000001
  title varchar(500) [not null]
  space_id varchar(50) [not null]
  parent_page_id varchar(50) // nullable for root pages
  body_storage text // 
  // body_view text // rendered HTML
  status enum('current','draft','locked', 'archived','deleted') [not null, default: 'current']
  // version_number int [not null, default: 1]
  created_by varchar(50) [not null]
  created_at timestamp [not null, default: `NOW()`]
  updated_by varchar(50) [not null]
  updated_at timestamp [not null, default: `NOW()`]

  indexes {
    parent_page_id
    space_id
    // folder_id
    (space_id, status)
    // page_number [unique]
  }
}

Table databases {
  database_id varchar(50) [primary key]
  title varchar(500) [not null]
  host_space_id varchar(50)      // For space-level databases
  host_page_id varchar(50)       // For page-level databases
  // Either host_space_id OR host_page_id must be set (mutually exclusive)
  description text
  status enum('current','archived','deleted') [not null, default: 'current']
  created_by varchar(50) [not null]
  created_at timestamp [not null, default: `NOW()`]
  updated_by varchar(50) [not null]
  updated_at timestamp [not null, default: `NOW()`]
  
  indexes {
    host_space_id
    host_page_id
  }
}

Table whiteboards {
  whiteboard_id varchar(50) [primary key]
  title varchar(500) [not null]
  host_space_id varchar(50)      // For space-level whiteboards
  host_page_id varchar(50)       // For page-level whiteboards  
  content text
  status enum('current','archived','deleted', 'locked') [not null, default: 'current']
  created_by varchar(50) [not null]
  created_at timestamp [not null, default: `NOW()`]
  updated_by varchar(50) [not null]
  updated_at timestamp [not null, default: `NOW()`]
  
  indexes {
    host_space_id
    host_page_id
  }
}

Table smart_links {
  smart_link_id varchar(50) [primary key]
  title varchar(500) [not null]
  url varchar(2000) [not null]
  
  // What this smart link POINTS TO (the reference/target)
  target_id varchar(50)           // The entity this link references
  target_type enum('page','database','whiteboard','external', 'attachment')
  
  // Where this smart link is DISPLAYED (the container)
  host_page_id varchar(50) [not null]
  // host_id varchar(50) [not null]
  // host_type enum('page','database','whiteboard') [not null]
  
  // link_type varchar(100) // e.g., 'jira_issue', 'external_url', etc.
  // link_type enum(
  //   'internal_link',
  //   'document_link',
  //   'repository_link',
  //   'communication_link',
  //   'media_link',
  //   'external_url'
  // ) [not null, default: 'external_url']
  // status enum('current','archived','deleted') [not null, default: 'current']
  created_by varchar(50) [not null]
  created_at timestamp [not null, default: `NOW()`]
  updated_by varchar(50) [not null]
  updated_at timestamp [not null, default: `NOW()`]
  
  indexes {
    // parent_id
    // space_id
    url
  }
}


// ============================================================================
// SPACES
// ============================================================================

Table spaces {
  space_id varchar(50) [primary key]
  space_key varchar(50) [not null, unique]
  name varchar(255) [not null]
  description text
  type enum('global','personal') [not null]
  status enum('current','archived') [not null, default: 'current']
  // homepage_id varchar(50) // FK to pages
  created_by varchar(50) [not null]
  created_at timestamp [not null, default: `NOW()`]
  updated_at timestamp [not null, default: `NOW()`]
  
  indexes {
    space_key [unique]
  }
}

// ============================================================================
// USERS AND AUTHENTICATION
// ============================================================================

Table users {
  user_id varchar(50) [primary key]
  // account_id varchar(100) [not null, unique] // Atlassian account ID
  email varchar(320) [not null, unique]
  display_name varchar(255) [not null]
  // account_type enum('atlassian','app','anonymous') [not null]
  status enum('active','inactive','deactivated') [not null, default: 'active']
  created_at timestamp [not null, default: `NOW()`]
  updated_at timestamp [not null, default: `NOW()`]
  
  indexes {
    // account_id [unique]
    email [unique]
  }
}


// ============================================================================
// ATTACHMENTS
// ============================================================================

Table attachments {
  attachment_id varchar(50) [primary key]
  content_id varchar(50) [not null]
  content_type enum('page','database','whiteboard','smart_link') [not null]
  host_page_id varchar(50) 
  file_name varchar(500) [not null]
  file_url varchar(2000) [not null]
  // file_size_bytes bigint [not null]
  // media_type varchar(100) [not null] // MIME type
  // download_url varchar(2000) [not null]
  // comment text
  // version_number int [not null, default: 1]
  status enum('current','archived','deleted') [not null, default: 'current']
  uploaded_by varchar(50) [not null]
  uploaded_at timestamp [not null, default: `NOW()`]
  updated_at timestamp [not null, default: `NOW()`]
  
  indexes {
    content_id
    (content_id, content_type)
    uploaded_by
  }
}


// ============================================================================
// VERSIONS
// ============================================================================


Table page_versions {
  page_version_id varchar(50) [primary key]
  page_id varchar(50) [not null]
  version_number int [not null]
  title varchar(500) [not null]
  body_storage text
  // is_minor_edit boolean [not null, default: false]
  // created_by varchar(50)  
  created_at timestamp [not null, default: `NOW()`]
}

Table page_version_components {
  component_id varchar(50) [primary key]
  page_version_id varchar(50) [not null]
  component_type enum('whiteboard','smart_link') [not null]
  component_data text [not null] // Complete snapshot of the component
  
  indexes {
    page_version_id
  }
}

Ref: page_version_components.page_version_id > page_versions.page_version_id


// Table whiteboard_versions {
//   whiteboard_version_id varchar(50) [primary key]
//   whiteboard_id varchar(50) [not null]
//   version_number int [not null]
//   title varchar(500) [not null]
//   content text
//   version_message text
//   created_by varchar(50) [not null]
//   created_at timestamp [not null, default: `NOW()`]
  
//   indexes {
//     whiteboard_id
//     (whiteboard_id, version_number) [unique]
//   }
// }



// ============================================================================
// LABELS
// ============================================================================

Table page_labels {
  page_label_id varchar(50) [primary key]
  page_id varchar(50) [not null]
  label_name varchar(255) [not null]  // stored directly
  added_by varchar(50) [not null]
  added_at timestamp [not null, default: `NOW()`]
  
  indexes {
    page_id
    (page_id, label_name) [unique]
  }
}

// Table labels {
//   label_id varchar(50) [primary key]
//   label_name varchar(255) [not null, unique]
//   // prefix varchar(50) // e.g., 'global', 'my', 'team'
//   created_at timestamp [not null, default: `NOW()`]
  
//   indexes {
//     label_name
//   }
// }

// Table page_labels {
//   page_label_id varchar(50) [primary key]
//   page_id varchar(50) [not null]
//   label_id varchar(50) [not null]
//   added_by varchar(50) [not null]
//   added_at timestamp [not null, default: `NOW()`]
  
//   indexes {
//     (page_id, label_id) [unique]
//     label_id
//   }
// }

// Table attachment_labels {
//   attachment_label_id varchar(50) [primary key]
//   attachment_id varchar(50) [not null]
//   label_id varchar(50) [not null]
//   added_by varchar(50) [not null]
//   added_at timestamp [not null, default: `NOW()`]
  
//   indexes {
//     (attachment_id, label_id) [unique]
//     label_id
//   }
// }

// ============================================================================
// LIKES
// ============================================================================

// Table likes {
//   like_id varchar(50) [primary key]
//   page_id varchar(50) [not null]
//   // content_id varchar(50) [not null]
//   // content_type enum('page','database','whiteboard','smart_link') [not null]
//   user_id varchar(50) [not null]
//   // account_id varchar(100) [not null] // Atlassian account ID
//   liked_at timestamp [not null, default: `NOW()`]
  
//   indexes {
//     // (content_id, content_type, user_id) [unique]
//     // content_id
//     user_id
//   }
// }

// ============================================================================
// PERMISSIONS
// ============================================================================

Table permissions {
  permission_id varchar(50) [primary key]
  content_id varchar(50) [not null]
  content_type enum('space','page') [not null]
  user_id varchar(50)
  operation enum('view','edit','delete','create','admin', 'restrict_other_users') [not null]
  granted_by varchar(50)
  granted_at timestamp [not null, default: `NOW()`]
  
  indexes {
    (content_id, content_type)
    user_id
    operation
  }
}


// ============================================================================
// RELATIONSHIPS
// ============================================================================

// Content Pages
Ref: pages.parent_page_id > pages.page_id
Ref: pages.space_id > spaces.space_id
Ref: pages.created_by > users.user_id
Ref: pages.updated_by > users.user_id

// Databases
Ref: databases.host_space_id > spaces.space_id
Ref: databases.host_page_id > pages.page_id
Ref: databases.created_by > users.user_id
Ref: databases.updated_by > users.user_id

// Whiteboards
Ref: whiteboards.host_space_id > spaces.space_id
Ref: whiteboards.host_page_id > pages.page_id
Ref: whiteboards.created_by > users.user_id
Ref: whiteboards.updated_by > users.user_id

// Smart Links
// Ref: smart_links.space_id > spaces.space_id
Ref: smart_links.created_by > users.user_id
Ref: smart_links.updated_by > users.user_id

// Spaces
// Ref: spaces.homepage_id > pages.page_id
Ref: spaces.created_by > users.user_id

// Attachments
Ref: attachments.uploaded_by > users.user_id
// Ref: attachment_bodies.attachment_version_id > attachment_versions.attachment_version_id

// Versions
Ref: page_versions.page_id > pages.page_id
// Ref: page_versions.created_by > users.user_id

// Labels
Ref: page_labels.page_id > pages.page_id
// Ref: page_labels.label_id > labels.label_id
Ref: page_labels.added_by > users.user_id

// Ref: attachment_labels.attachment_id > attachments.attachment_id
// Ref: attachment_labels.label_id > labels.label_id
// Ref: attachment_labels.added_by > users.user_id


// Likes
// Ref: likes.user_id > users.user_id

// Permissions
Ref: permissions.user_id > users.user_id
// Ref: permissions.group_id > groups.group_id
Ref: permissions.granted_by > users.user_id

Ref: smart_links.host_page_id > pages.page_id


You have to provide the json file for each function separately including all the tests for that function in the actions part. You are going to provide the bash script that will create and populate those json files in a folder named tools_regression_tests/interface_3/.

Note: ids are just numeric strings "1", "2", ...
